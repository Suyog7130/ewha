{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Part 3B \u2014 MNIST CNN with TensorFlow (GradientTape)\n_Last updated: 2025-11-16_\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\nThis notebook uses **TensorFlow Core** with an explicit training loop (`tf.GradientTape`).  \nWe still use Keras layers for convenience, but control the optimization ourselves.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Environment"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nimport os, sys, time, json, pathlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nprint(\"TensorFlow:\", tf.__version__)\nprint(\"Devices:\", tf.config.list_physical_devices())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## Dataset & preprocessing\n\nWe use **MNIST** (28\u00d728 grayscale digits, 10 classes). Input is normalized to `[0, 1]`\nand reshaped to `(N, 28, 28, 1)` for CNNs. Labels are integers `0..9`.\n\nWe also create a `tf.data.Dataset` pipeline."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nfrom tensorflow.keras.datasets import mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.astype(\"float32\")/255.0\nx_test  = x_test.astype(\"float32\")/255.0\nx_train = np.expand_dims(x_train, -1)\nx_test  = np.expand_dims(x_test, -1)\n\nBATCH=128\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(BATCH)\nval_ds   = tf.data.Dataset.from_tensor_slices((x_test,  y_test)).batch(BATCH)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Model + custom train loop"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef build_model():\n    inputs = keras.Input(shape=(28,28,1))\n    x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n    x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Dropout(0.25)(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(128, activation=\"relu\")(x)\n    outputs = layers.Dense(10, activation=\"softmax\")(x)\n    return keras.Model(inputs, outputs)\n\nmodel = build_model()\noptimizer = keras.optimizers.Adam(1e-3)\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\ntrain_loss_hist, val_loss_hist, val_acc_hist = [], [], []\n\n@tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        logits = model(x, training=True)\n        loss = loss_fn(y, logits)\n    grads = tape.gradient(loss, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    return loss\n\n@tf.function\ndef val_step(x, y):\n    preds = model(x, training=False)\n    loss = loss_fn(y, preds)\n    acc  = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(preds, axis=1), tf.cast(y, tf.int64)), tf.float32))\n    return loss, acc\n\nEPOCHS=5\nt0=time.perf_counter()\nfor epoch in range(EPOCHS):\n    # train\n    lsum=0.0; n=0\n    for x,y in train_ds:\n        l = train_step(x,y)\n        lsum += float(l); n += 1\n    train_loss_hist.append(lsum/max(n,1))\n    # val\n    lsum=0.0; asum=0.0; n=0\n    for x,y in val_ds:\n        l,a = val_step(x,y)\n        lsum += float(l); asum += float(a); n += 1\n    val_loss_hist.append(lsum/max(n,1))\n    val_acc_hist.append(asum/max(n,1))\n    print(f\"Epoch {epoch+1}/{EPOCHS} - loss {train_loss_hist[-1]:.4f} - val_loss {val_loss_hist[-1]:.4f} - val_acc {val_acc_hist[-1]:.4f}\")\ntrain_time = time.perf_counter()-t0\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Curves"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nplt.figure(figsize=(6,4))\nplt.plot(train_loss_hist, label=\"train_loss\")\nplt.plot(val_loss_hist, label=\"val_loss\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True); plt.title(\"TF Core: loss\")\nplt.tight_layout()\nplt.savefig(\"artifacts/tfcore_loss.png\", dpi=150)\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.plot(val_acc_hist, label=\"val_accuracy\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.grid(True); plt.title(\"TF Core: val accuracy\")\nplt.tight_layout()\nplt.savefig(\"artifacts/tfcore_acc.png\", dpi=150)\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Evaluate & save metrics"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Utility: save artifacts (plots & metrics)\nimport os, json, pathlib, time\nART = pathlib.Path(\"artifacts\")\nART.mkdir(exist_ok=True)\n\ndef save_metrics(name, **metrics):\n    path = ART / f\"{name}_metrics.json\"\n    with open(path, \"w\") as f:\n        json.dump(metrics, f, indent=2)\n    print(\"Saved:\", path)\n\ndef effective_loc(*funcs):\n    import inspect\n    n = 0\n    for f in funcs:\n        try:\n            src = inspect.getsource(f)\n            for line in src.splitlines():\n                s = line.strip()\n                if s and not s.startswith(\"#\"):\n                    n += 1\n        except Exception:\n            pass\n    return n\n\n# Final evaluation on test set\ntest_logits = model.predict(x_test, verbose=0)\ntest_preds = test_logits.argmax(axis=1)\ntest_acc = float(np.mean(test_preds == y_test))\ntest_loss = float(tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_test, test_logits)).numpy())\n\nsave_metrics(\"tfcore\", framework=\"tensorflow\", test_accuracy=test_acc, test_loss=test_loss,\n             train_time_sec=float(train_time), params=int(model.count_params()), epochs=len(val_loss_hist),\n             device=str(tf.config.list_physical_devices(\"GPU\") or tf.config.list_physical_devices(\"CPU\")),\n             effective_loc=int(effective_loc(build_model, train_step, val_step)))\nmodel.save(\"artifacts/tensorflow_mnist\")\nprint(\"Saved artifacts/ and metrics.\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "Part 3B \u2014 MNIST CNN with TensorFlow (GradientTape)",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}