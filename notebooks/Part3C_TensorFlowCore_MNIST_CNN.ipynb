{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e61a582",
   "metadata": {},
   "source": [
    "# Part 3B — MNIST CNN with TensorFlow (GradientTape)\n",
    "_Last updated: 2025-11-16_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137271c",
   "metadata": {},
   "source": [
    "\n",
    "This notebook uses **TensorFlow Core** with an explicit training loop (`tf.GradientTape`).  \n",
    "We still use Keras layers for convenience, but control the optimization ourselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d13ab",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff22ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, time, json, pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Devices:\", tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7899e6",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset & preprocessing\n",
    "\n",
    "We use **MNIST** (28×28 grayscale digits, 10 classes). Input is normalized to `[0, 1]`\n",
    "and reshaped to `(N, 28, 28, 1)` for CNNs. Labels are integers `0..9`.\n",
    "\n",
    "We also create a `tf.data.Dataset` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\")/255.0\n",
    "x_test  = x_test.astype(\"float32\")/255.0\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "\n",
    "BATCH=128\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(BATCH)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((x_test,  y_test)).batch(BATCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280f5db",
   "metadata": {},
   "source": [
    "## Model + custom train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b494b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(28,28,1))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "model = build_model()\n",
    "optimizer = keras.optimizers.Adam(1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "train_loss_hist, val_loss_hist, val_acc_hist = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f99014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(x, y):\n",
    "    preds = model(x, training=False)\n",
    "    loss = loss_fn(y, preds)\n",
    "    acc  = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(preds, axis=1), tf.cast(y, tf.int64)), tf.float32))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a7de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EPOCHS=5\n",
    "t0=time.perf_counter()\n",
    "for epoch in range(EPOCHS):\n",
    "    # train\n",
    "    lsum=0.0; n=0\n",
    "    for x,y in train_ds:\n",
    "        l = train_step(x,y)\n",
    "        lsum += float(l); n += 1\n",
    "    train_loss_hist.append(lsum/max(n,1))\n",
    "    # val\n",
    "    lsum=0.0; asum=0.0; n=0\n",
    "    for x,y in val_ds:\n",
    "        l,a = val_step(x,y)\n",
    "        lsum += float(l); asum += float(a); n += 1\n",
    "    val_loss_hist.append(lsum/max(n,1))\n",
    "    val_acc_hist.append(asum/max(n,1))\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - loss {train_loss_hist[-1]:.4f} - val_loss {val_loss_hist[-1]:.4f} - val_acc {val_acc_hist[-1]:.4f}\")\n",
    "train_time = time.perf_counter()-t0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d6ef0",
   "metadata": {},
   "source": [
    "## Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e514c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_loss_hist, label=\"train_loss\")\n",
    "plt.plot(val_loss_hist, label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True); plt.title(\"TF Core: loss\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"artifacts/tfcore_loss.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(val_acc_hist, label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.grid(True); plt.title(\"TF Core: val accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"artifacts/tfcore_acc.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7405434d",
   "metadata": {},
   "source": [
    "## Evaluate & save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility: save artifacts (plots & metrics)\n",
    "import os, json, pathlib, time\n",
    "ART = pathlib.Path(\"artifacts\")\n",
    "ART.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4589a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_metrics(name, **metrics):\n",
    "    path = ART / f\"{name}_metrics.json\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Saved:\", path)\n",
    "\n",
    "def effective_loc(*funcs):\n",
    "    import inspect\n",
    "    n = 0\n",
    "    for f in funcs:\n",
    "        try:\n",
    "            src = inspect.getsource(f)\n",
    "            for line in src.splitlines():\n",
    "                s = line.strip()\n",
    "                if s and not s.startswith(\"#\"):\n",
    "                    n += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    return n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final evaluation on test set\n",
    "test_logits = model.predict(x_test, verbose=0)\n",
    "test_preds = test_logits.argmax(axis=1)\n",
    "test_acc = float(np.mean(test_preds == y_test))\n",
    "test_loss = float(tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_test, test_logits)).numpy())\n",
    "\n",
    "save_metrics(\"tfcore\", framework=\"tensorflow\", test_accuracy=test_acc, test_loss=test_loss,\n",
    "             train_time_sec=float(train_time), params=int(model.count_params()), epochs=len(val_loss_hist),\n",
    "             device=str(tf.config.list_physical_devices(\"GPU\") or tf.config.list_physical_devices(\"CPU\")),\n",
    "             effective_loc=int(effective_loc(build_model, train_step, val_step)))\n",
    "model.save(\"artifacts/tensorflow_mnist\")\n",
    "print(\"Saved artifacts/ and metrics.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Part 3B — MNIST CNN with TensorFlow (GradientTape)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
