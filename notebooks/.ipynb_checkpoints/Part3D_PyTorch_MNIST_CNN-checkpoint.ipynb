{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Part 3C \u2014 MNIST CNN with PyTorch\n_Last updated: 2025-11-16_\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\nThis notebook implements a compact **PyTorch** CNN using `torchvision` datasets and a simple\ntraining loop. It auto-detects **CUDA**, **MPS** (Apple Silicon), or **CPU**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Environment"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nimport os, sys, time, json, pathlib, math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch, torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nprint(\"Torch:\", torch.__version__)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\nprint(\"Device:\", device)\nif device.type == \"cuda\":\n    torch.backends.cudnn.benchmark = True\ntry:\n    torch.set_float32_matmul_precision(\"medium\")\nexcept Exception:\n    pass\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## Dataset & preprocessing\n\nWe use **MNIST** (28\u00d728 grayscale digits, 10 classes). Input is normalized to `[0, 1]`\nand reshaped to `(N, 28, 28, 1)` for CNNs. Labels are integers `0..9`.\n\nWe use `torchvision.datasets.MNIST` and standard normalization."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ntfm = transforms.Compose([transforms.ToTensor()])  # converts to [0,1] and adds channel dim\ntrain_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=tfm)\ntest_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=tfm)\ntrain_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=(device.type==\"cuda\"))\ntest_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=(device.type==\"cuda\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Model + training loop"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(1,32,3), nn.ReLU(),\n            nn.Conv2d(32,64,3), nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.25),\n            nn.Flatten(),\n            nn.Linear(9216,128), nn.ReLU(),\n            nn.Linear(128,10)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nmodel = Net().to(device)\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\ndef train_epoch():\n    model.train()\n    ls=0.0; n=0\n    for x,y in train_loader:\n        x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)\n        opt.zero_grad()\n        logits = model(x)\n        loss = loss_fn(logits, y)\n        loss.backward()\n        opt.step()\n        ls += float(loss.item()); n += 1\n    return ls/max(n,1)\n\ndef evaluate():\n    model.eval()\n    correct=0; total=0; ls=0.0; n=0\n    with torch.no_grad():\n        for x,y in test_loader:\n            x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)\n            logits = model(x)\n            loss = loss_fn(logits, y)\n            pred = logits.argmax(1)\n            correct += int((pred==y).sum().item()); total += int(y.numel())\n            ls += float(loss.item()); n += 1\n    return ls/max(n,1), correct/total\n\nEPOCHS=5\nt0=time.perf_counter()\ntrain_hist=[]; val_hist=[]\nfor epoch in range(EPOCHS):\n    tl=train_epoch(); vl,va=evaluate()\n    train_hist.append(tl); val_hist.append(vl)\n    print(f\"Epoch {epoch+1}/{EPOCHS} - loss {tl:.4f} - val_loss {vl:.4f} - val_acc {va:.4f}\")\ntrain_time=time.perf_counter()-t0\ntest_loss, test_acc = evaluate()\nprint(\"Test acc:\", test_acc)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Curves"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nplt.figure(figsize=(6,4))\nplt.plot(train_hist, label=\"train_loss\")\nplt.plot(val_hist, label=\"val_loss\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True); plt.title(\"PyTorch: loss\")\nplt.tight_layout()\nplt.savefig(\"artifacts/pytorch_loss.png\", dpi=150)\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Save metrics & model"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Utility: save artifacts (plots & metrics)\nimport os, json, pathlib, time\nART = pathlib.Path(\"artifacts\")\nART.mkdir(exist_ok=True)\n\ndef save_metrics(name, **metrics):\n    path = ART / f\"{name}_metrics.json\"\n    with open(path, \"w\") as f:\n        json.dump(metrics, f, indent=2)\n    print(\"Saved:\", path)\n\ndef effective_loc(*funcs):\n    import inspect\n    n = 0\n    for f in funcs:\n        try:\n            src = inspect.getsource(f)\n            for line in src.splitlines():\n                s = line.strip()\n                if s and not s.startswith(\"#\"):\n                    n += 1\n        except Exception:\n            pass\n    return n\n\nsave_metrics(\"pytorch\", framework=\"pytorch\", test_accuracy=float(test_acc), test_loss=float(test_loss),\n             train_time_sec=float(train_time), params=int(sum(p.numel() for p in model.parameters())),\n             epochs=len(train_hist), device=str(device),\n             effective_loc=int(effective_loc(Net, train_epoch, evaluate)))\ntorch.save(model.state_dict(), \"artifacts/pytorch_mnist.pt\")\nprint(\"Saved artifacts/ and metrics.\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "Part 3C \u2014 MNIST CNN with PyTorch",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}