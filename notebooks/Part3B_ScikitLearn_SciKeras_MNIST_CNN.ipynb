{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Part 3D \u2014 MNIST CNN with scikit\u2011learn (via SciKeras)\n_Last updated: 2025-11-16_\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n`scikit-learn` doesn\u2019t implement CNNs natively, so we use the **SciKeras** wrapper to expose a Keras\nmodel through the familiar `fit/score` API (and GridSearchCV). This still runs with TensorFlow under the hood.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Environment"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nimport os, sys, time, json, pathlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import mnist\nfrom scikeras.wrappers import KerasClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nprint(\"TensorFlow:\", tf.__version__)\nprint(\"Devices:\", tf.config.list_physical_devices())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## Dataset & preprocessing\n\nWe use **MNIST** (28\u00d728 grayscale digits, 10 classes). Input is normalized to `[0, 1]`\nand reshaped to `(N, 28, 28, 1)` for CNNs. Labels are integers `0..9`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx = np.concatenate([x_train, x_test]).astype(\"float32\")/255.0\ny = np.concatenate([y_train, y_test]).astype(\"int64\")\nx = np.expand_dims(x, -1)\n\nXtr, Xte, ytr, yte = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n\ndef make_model(learning_rate=1e-3):\n    inputs = keras.Input(shape=(28,28,1))\n    x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n    x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n    x = layers.MaxPooling2D()(x)\n    x = layers.Dropout(0.25)(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(128, activation=\"relu\")(x)\n    outputs = layers.Dense(10, activation=\"softmax\")(x)\n    model = models.Model(inputs, outputs)\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate),\n                  loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\nclf = KerasClassifier(model=make_model, epochs=5, batch_size=128, verbose=0)\n# Optional: quick hyperparam search (tiny grid for demo)\nparam_grid = {\"model__learning_rate\":[1e-3, 5e-4]}\nt0=time.perf_counter()\ngrid = GridSearchCV(clf, param_grid=param_grid, cv=3, n_jobs=1)\ngrid.fit(Xtr, ytr)\ntrain_time=time.perf_counter()-t0\n\nprint(\"Best params:\", grid.best_params_)\ny_pred = grid.predict(Xte)\ntest_acc = accuracy_score(yte, y_pred)\nprint(\"Test accuracy:\", test_acc)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Plot training history (from the best estimator)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nbest = grid.best_estimator_\nhist = best.model_.history.history if hasattr(best.model_, \"history\") else None\nif hist:\n    plt.figure(figsize=(6,4))\n    if \"loss\" in hist: plt.plot(hist[\"loss\"], label=\"train_loss\")\n    if \"val_loss\" in hist: plt.plot(hist[\"val_loss\"], label=\"val_loss\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True); plt.title(\"SciKeras: loss\")\n    plt.tight_layout()\n    plt.savefig(\"artifacts/sklearn_loss.png\", dpi=150)\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Save metrics"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Utility: save artifacts (plots & metrics)\nimport os, json, pathlib, time\nART = pathlib.Path(\"artifacts\")\nART.mkdir(exist_ok=True)\n\ndef save_metrics(name, **metrics):\n    path = ART / f\"{name}_metrics.json\"\n    with open(path, \"w\") as f:\n        json.dump(metrics, f, indent=2)\n    print(\"Saved:\", path)\n\ndef effective_loc(*funcs):\n    import inspect\n    n = 0\n    for f in funcs:\n        try:\n            src = inspect.getsource(f)\n            for line in src.splitlines():\n                s = line.strip()\n                if s and not s.startswith(\"#\"):\n                    n += 1\n        except Exception:\n            pass\n    return n\n\nparams = best.model_.count_params() if hasattr(best, \"model_\") else None\nsave_metrics(\"sklearn\", framework=\"scikit-learn+scikeras\", test_accuracy=float(test_acc),\n             test_loss=float(\"nan\"), train_time_sec=float(train_time), params=int(params or 0),\n             epochs=int(len(hist[\"loss\"])) if hist else 0,\n             device=str(tf.config.list_physical_devices(\"GPU\") or tf.config.list_physical_devices(\"CPU\")),\n             effective_loc=int(effective_loc(make_model)))\nprint(\"Saved artifacts/ and metrics.\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "Part 3D \u2014 MNIST CNN with scikit\u2011learn (via SciKeras)",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}